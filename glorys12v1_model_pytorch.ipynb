{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85407f2e-46fd-40a8-badb-e327fb94034d",
   "metadata": {},
   "source": [
    "# Apply neural network model to GLORYS12V1 daily data\n",
    "Created by Ivan Lima on Fri May  6 2022 15:34:42 -0400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c762dd13-92f4-458f-8af2-d8af1c5ceeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated on Mon Jul 11 12:47:25 2022\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, datetime\n",
    "from tqdm import notebook\n",
    "print('Last updated on {}'.format(datetime.datetime.now().ctime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe47f5-1ca4-45fb-bda7-13b9cc7cb0b9",
   "metadata": {},
   "source": [
    "## Load neural network model and data scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e389db-51a4-440f-ae7d-d5c457258a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda3/envs/py39/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPReg(\n",
       "  (l1): Linear(in_features=5, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (activ): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, joblib\n",
    "import torch.nn as nn\n",
    "\n",
    "scaler = joblib.load('models/scaler_nosat.joblib')\n",
    "\n",
    "features = ['depth', 'bottom_depth', 'Temperature', 'Salinity', 'pCO2_monthave']\n",
    "\n",
    "n_features = len(features) # number of input variables\n",
    "n_targets = 2  # number of output variables\n",
    "n_hidden = 256 # number of hidden layers\n",
    "learning_rate = 0.001\n",
    "\n",
    "class MLPReg(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_targets):\n",
    "        super(MLPReg, self).__init__()\n",
    "        self.l1    = nn.Linear(n_features, n_hidden)\n",
    "        self.l2    = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l3    = nn.Linear(n_hidden, n_targets)\n",
    "        self.activ = nn.LeakyReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.activ(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.activ(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "nn_reg = MLPReg(n_features=n_features, n_hidden=n_hidden, n_targets=n_targets) # create model instance\n",
    "loss_func = nn.MSELoss()                                                       # loss function (mean square error)\n",
    "optimizer = torch.optim.Adam(nn_reg.parameters(), lr=learning_rate)            # optimizer\n",
    "\n",
    "nn_reg.load_state_dict(torch.load('models/nn_reg_nosat_state.pth'))\n",
    "nn_reg.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da3627-ce0a-44be-9e79-830aded2862e",
   "metadata": {},
   "source": [
    "## Extract bottom depth at grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26207d8c-f245-4c79-9c13-673aeba75d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 5313.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>bottom_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-78.000000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-77.916664</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.997803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-77.833336</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.000732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-77.750000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-77.666664</td>\n",
       "      <td>34.0</td>\n",
       "      <td>18.000275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  bottom_depth\n",
       "0 -78.000000      34.0      4.000000\n",
       "1 -77.916664      34.0     10.997803\n",
       "2 -77.833336      34.0      3.000732\n",
       "3 -77.750000      34.0     11.000000\n",
       "4 -77.666664      34.0     18.000275"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "ds_grid = xr.open_dataset('/bali/data/ilima/GLORYS12V1/daily/GLORYS12V1_NW_Atlantic_1993_daily.nc',\n",
    "                          drop_variables = ['mlotst','zos','bottomT'])\n",
    "\n",
    "xx, yy = np.meshgrid(ds_grid.longitude.values, ds_grid.latitude.values)\n",
    "df_positions = pd.DataFrame({'longitude': xx.ravel(), 'latitude': yy.ravel()})\n",
    "\n",
    "# read bottom topography data\n",
    "etopo = xr.open_dataset('data/etopo5.nc', chunks='auto')\n",
    "# etopo['bath'] = etopo.bath.where(etopo.bath<0) # ocean points only\n",
    "etopo = etopo.isel(X=slice(3100,4000), Y=slice(1300,1700)) # subset data to make things faster\n",
    "\n",
    "X = np.where(etopo.X>180, etopo.X-360, etopo.X) # 0:360 -> -180:180\n",
    "lon_topo, lat_topo = np.meshgrid(X, etopo.Y.values)\n",
    "lon, lat = df_positions.longitude.values, df_positions.latitude.values\n",
    "bottom_depth = griddata((lon_topo.ravel(), lat_topo.ravel()), etopo.bath.values.ravel(), (lon, lat), method='linear')\n",
    "df_positions['bottom_depth'] = np.abs(bottom_depth)\n",
    "print(df_positions.bottom_depth.min(), df_positions.bottom_depth.max())\n",
    "df_positions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a1176-da86-49b4-b01a-7dbe518d4838",
   "metadata": {},
   "source": [
    "## Read monthly atmospheric pCO2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cf4a12-923c-4ffe-b281-f2475aa65068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pco2_monthly = pd.read_csv('work/co2_mm_mlo.csv')\n",
    "df_pco2_monthly = df_pco2_monthly.set_index(['year','month'])\n",
    "\n",
    "# for i in df_pco2_monthly.loc[2016].index:\n",
    "#     print(i, df_pco2_monthly.loc[(2016,i),'average'])\n",
    "# atm_pco2 = [df_pco2_monthly.loc[(2016,i),'average'] for i in ds.time.dt.month.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9af85f-75ce-4bdd-8fba-5df7b3bf9f07",
   "metadata": {},
   "source": [
    "## Apply NN model to GLORYS12V1 data and compute carbonate chemistry variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5b9872-3cd8-4e13-8055-0ab3a5a947f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7616e446a0504f57bb58820307beb6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PyCO2SYS as pyco2\n",
    "import gsw\n",
    "\n",
    "cols = ['time', 'longitude', 'latitude', 'depth', 'bottom_depth', 'Temperature', 'Salinity', 'pCO2_monthave']\n",
    "\n",
    "year = 2019\n",
    "ds_in = xr.open_dataset('/bali/data/ilima/GLORYS12V1/daily/GLORYS12V1_NW_Atlantic_{}_daily.nc'.format(year),\n",
    "                        drop_variables = ['mlotst','zos','bottomT'])\n",
    "ds_out = [] # store list of datasets\n",
    "\n",
    "for t in notebook.tqdm(range(ds_in.dims['time'])):\n",
    "    ds = ds_in.isel(time=t)\n",
    "\n",
    "    # add monthly atmospheric pCO2\n",
    "    for i in df_pco2_monthly.loc[year].index:\n",
    "        if i==1:\n",
    "            fill = np.nan\n",
    "        else:\n",
    "            fill = ds.pCO2_monthave\n",
    "\n",
    "        ds['pCO2_monthave'] = xr.where(ds.time.dt.month==i, df_pco2_monthly.loc[(year,i),'average'], fill)\n",
    "\n",
    "\n",
    "    # convert dataset to dataframe & rename variables\n",
    "    df_glorys = ds[['pCO2_monthave','thetao','so']].to_dataframe()\n",
    "    df_glorys = df_glorys.reset_index().rename(columns={'thetao':'Temperature', 'so':'Salinity'})\n",
    "    # merge bottom depth with GLORYS12V1 data\n",
    "    df_glorys = pd.merge(df_positions, df_glorys, on=['longitude', 'latitude'])\n",
    "    df_data = df_glorys[cols].dropna()\n",
    "    # print('{:,d} data points\\n'.format(df_data.shape[0]))\n",
    "\n",
    "    X_numpy = df_data[features].values # select features\n",
    "    X_numpy_scaled = scaler.transform(X_numpy) # rescale features\n",
    "    X = torch.from_numpy(X_numpy_scaled.astype(np.float32)) # convert array to tensor\n",
    "\n",
    "    # apply model to rescaled features\n",
    "    with torch.no_grad():\n",
    "        Y_pred = nn_reg(X)\n",
    "\n",
    "    # add estimated DIC & TA to features dataframe\n",
    "    df_data['DIC'] = Y_pred[:,0]\n",
    "    df_data['TA'] = Y_pred[:,1]\n",
    "\n",
    "    # compute additional carbonate chemistry variables\n",
    "    pressure =  gsw.p_from_z(-df_data.depth.values, df_data.latitude.values)\n",
    "    kwargs = dict(\n",
    "        par1 = df_data.TA.values,  # TA\n",
    "        par2 = df_data.DIC.values, # DIC\n",
    "        par1_type = 1,             # type 1 = alkalinity\n",
    "        par2_type = 2,             # type 2 = DIC\n",
    "        salinity = df_data.Salinity.values,\n",
    "        temperature = df_data.Temperature.values,\n",
    "        pressure = pressure,\n",
    "        opt_k_carbonic = 10,  # LDK00, Lueker et al 2000\n",
    "        opt_k_bisulfate = 1,  # D90a, Dickson 1990\n",
    "        opt_total_borate = 2, # LKB10, Lee et al 2010\n",
    "        opt_k_fluoride = 2    # PF87, Perez & Fraga 1987\n",
    "    )\n",
    "    results = pyco2.sys(**kwargs)\n",
    "    co2sys_vars = ['pH', 'saturation_calcite', 'saturation_aragonite']\n",
    "    for vname in co2sys_vars:\n",
    "        df_data[vname] = results[vname]\n",
    "    \n",
    "    # merge estimated carbonate chemistry variables to original dataframe\n",
    "    for vname in ['DIC','TA'] + co2sys_vars:\n",
    "        df_glorys[vname] = df_data[vname]\n",
    "\n",
    "    # convert dataframe to xarray dataset\n",
    "    df = df_glorys.set_index(['time','depth','latitude','longitude'])\n",
    "    ds_bgc = df[['Temperature', 'Salinity', 'DIC', 'TA','pH', 'saturation_calcite', 'saturation_aragonite']].to_xarray()\n",
    "\n",
    "    ds_out.append(ds_bgc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f90e8-6b2a-4754-b10c-4ac36ad59b23",
   "metadata": {},
   "source": [
    "## Create output dataset including T, S, DIC & TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78bd3420-6465-46cd-a815-eb6422d6188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_attrs = {\n",
    "    'DIC': {'long_name': 'Dissolved inorganic carbon',\n",
    "            'standard_name': 'DIC',\n",
    "            'units': 'micro mol/kg',\n",
    "            'unit_long': 'micro mol/kg'},\n",
    "    'TA': {'long_name': 'Total alkalinity',\n",
    "            'standard_name': 'TA',\n",
    "            'units': 'micro mol/kg',\n",
    "            'unit_long': 'micro mol/kg'},\n",
    "    'pH': {'long_name': 'Total pH',\n",
    "            'standard_name': 'pH',\n",
    "            'units': '',\n",
    "            'unit_long': ''},\n",
    "    'saturation_calcite': {'long_name': 'Calcite saturation state',\n",
    "            'standard_name': 'Calcite saturation',\n",
    "            'units': '',\n",
    "            'unit_long': ''},\n",
    "    'saturation_aragonite': {'long_name': 'Aragonite saturation state',\n",
    "            'standard_name': 'Aragonite saturation',\n",
    "            'units': '',\n",
    "            'unit_long': ''},\n",
    "}\n",
    "\n",
    "ds_out = xr.concat(ds_out, dim='time')\n",
    "\n",
    "# copy variable attributes\n",
    "for vname in var_attrs:\n",
    "    ds_out[vname].attrs.update(var_attrs[vname])\n",
    "for attr in ['long_name','standard_name','units','unit_long']:\n",
    "    ds_out.Temperature.attrs[attr] = ds.thetao.attrs[attr]\n",
    "    ds_out.Salinity.attrs[attr] = ds.so.attrs[attr]\n",
    "    for vname in ['depth','latitude','longitude']:\n",
    "        ds_out[vname].attrs[attr] = ds[vname].attrs[attr]\n",
    "\n",
    "now = datetime.datetime.now().ctime()\n",
    "attrs={'contents':'Estimated carbonate chemistry variables for GLORYS12V1 output',\n",
    "       'history':'Created by Ivan Lima <ilima@whoi.edu> on {}'.format(now)}\n",
    "ds_out.attrs.update(attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf19dde-7c3a-4a0c-b1fa-70b1452a235b",
   "metadata": {},
   "source": [
    "## Save data into monthly files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb39d615-3d19-4bc5-b7fb-a53b39fad6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26364f69ef094513a50def45999c7d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-01_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-02_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-03_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-04_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-05_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-06_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-07_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-08_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-09_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-10_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-11_BGC.nc\n",
      "writing /bali/data/ilima/GLORYS12V1/daily/BGC/3D/GLORYS12V1_NW_Atlantic_2019-12_BGC.nc\n"
     ]
    }
   ],
   "source": [
    "outdir = '/bali/data/ilima/GLORYS12V1/daily/BGC/Model_3/3D'\n",
    "for mon in notebook.tqdm(range(1,13)):\n",
    "    outfile = os.path.join(outdir, 'GLORYS12V1_NW_Atlantic_{}-{:02d}_BGC.nc'.format(year, mon))\n",
    "    print('writing {}'.format(outfile))\n",
    "    ds_out.sel(time=ds_out.time.dt.month.isin([mon])).to_netcdf(outfile, mode='w', unlimited_dims=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd85872-c815-46dd-b2ed-381ea324a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 366/2\n",
    "# fig, axs = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(15, 10))\n",
    "# _ = ds_out.Temperature.isel(time=t, depth=0).plot(ax=axs[0,0])\n",
    "# _ = ds_out.Salinity.isel(time=t, depth=0).plot(ax=axs[0,1], robust=True)\n",
    "# _ = ds_out.DIC.isel(time=t, depth=0).plot(ax=axs[0,2], robust=True)\n",
    "# _ = ds_out.TA.isel(time=t, depth=0).plot(ax=axs[1,0], robust=True)\n",
    "# _ = ds_out.pH.isel(time=t, depth=0).plot(ax=axs[1,1], robust=True)\n",
    "# _ = ds_out.saturation_aragonite.isel(time=t, depth=0).plot(ax=axs[1,2], robust=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
