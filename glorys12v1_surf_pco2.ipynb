{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fae7d25-553e-4843-9130-be44d59f1855",
   "metadata": {},
   "source": [
    "# Compute surface pCO2 for GLORYS12V1 daily data\n",
    "Created by Ivan Lima on Tue May 10 2022 13:24:17 -0400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a1c26-4300-4004-9287-d0fb4e721eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, datetime\n",
    "from tqdm import notebook\n",
    "print('Last updated on {}'.format(datetime.datetime.now().ctime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311ade8-56ea-4dde-a0a3-13d4aa197393",
   "metadata": {},
   "source": [
    "## Load neural network model and data scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de25f3c-f17b-4b48-8297-ebd7be835146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, joblib\n",
    "import torch.nn as nn\n",
    "\n",
    "scaler = joblib.load('models/scaler_nosat.joblib')\n",
    "\n",
    "features = ['depth', 'bottom_depth', 'Temperature', 'Salinity', 'pCO2_monthave']\n",
    "\n",
    "n_features = len(features) # number of input variables\n",
    "n_targets = 2  # number of output variables\n",
    "n_hidden = 256 # number of hidden layers\n",
    "learning_rate = 0.001\n",
    "\n",
    "class MLPReg(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_targets):\n",
    "        super(MLPReg, self).__init__()\n",
    "        self.l1    = nn.Linear(n_features, n_hidden)\n",
    "        self.l2    = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l3    = nn.Linear(n_hidden, n_targets)\n",
    "        self.activ = nn.LeakyReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.activ(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.activ(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "nn_reg = MLPReg(n_features=n_features, n_hidden=n_hidden, n_targets=n_targets) # create model instance\n",
    "loss_func = nn.MSELoss()                                                       # loss function (mean square error)\n",
    "optimizer = torch.optim.Adam(nn_reg.parameters(), lr=learning_rate)            # optimizer\n",
    "\n",
    "nn_reg.load_state_dict(torch.load('models/nn_reg_nosat_state.pth'))\n",
    "nn_reg.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91072993-2ed0-46fe-9386-8dd9194b85f8",
   "metadata": {},
   "source": [
    "## Extract bottom depth at grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b8ed1-7222-4dc6-b4c5-8df5b09162db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "ds_grid = xr.open_dataset('/bali/data/ilima/GLORYS12V1/daily/GLORYS12V1_NW_Atlantic_1993_daily.nc',\n",
    "                          drop_variables = ['mlotst','zos','bottomT'])\n",
    "\n",
    "xx, yy = np.meshgrid(ds_grid.longitude.values, ds_grid.latitude.values)\n",
    "df_positions = pd.DataFrame({'longitude': xx.ravel(), 'latitude': yy.ravel()})\n",
    "\n",
    "# read bottom topography data\n",
    "etopo = xr.open_dataset('data/etopo5.nc', chunks='auto')\n",
    "# etopo['bath'] = etopo.bath.where(etopo.bath<0) # ocean points only\n",
    "etopo = etopo.isel(X=slice(3100,4000), Y=slice(1300,1700)) # subset data to make things faster\n",
    "\n",
    "X = np.where(etopo.X>180, etopo.X-360, etopo.X) # 0:360 -> -180:180\n",
    "lon_topo, lat_topo = np.meshgrid(X, etopo.Y.values)\n",
    "lon, lat = df_positions.longitude.values, df_positions.latitude.values\n",
    "bottom_depth = griddata((lon_topo.ravel(), lat_topo.ravel()), etopo.bath.values.ravel(), (lon, lat), method='linear')\n",
    "df_positions['bottom_depth'] = np.abs(bottom_depth)\n",
    "print(df_positions.bottom_depth.min(), df_positions.bottom_depth.max())\n",
    "df_positions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a7e1c-6632-4d48-ab40-9a9864d872ce",
   "metadata": {},
   "source": [
    "## Read monthly atmospheric pCO2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9adac-7b25-4657-b7c8-b9ffa14e6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pco2_monthly = pd.read_csv('work/co2_mm_mlo.csv')\n",
    "df_pco2_monthly = df_pco2_monthly.set_index(['year','month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b83074-4e56-43f1-8507-65d971841404",
   "metadata": {},
   "source": [
    "## Apply NN model to GLORYS12V1 data and compute surface pCO2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386f5de-1745-4a39-a433-01e90d2dadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyCO2SYS as pyco2\n",
    "import gsw\n",
    "\n",
    "var_attrs = {\n",
    "    'SST': {'long_name': 'Sea surface temperature',\n",
    "            'standard_name': 'sea_water_potential_temperature',\n",
    "            'units': 'degrees C',\n",
    "            'unit_long': 'Degrees Celsius'},\n",
    "    'SSS': {'long_name': 'Sea surface salinity',\n",
    "            'standard_name': 'sea_water_salinity',\n",
    "            'units': '1e-3',\n",
    "            'unit_long': 'Practical Salinity Unit'},\n",
    "    'DIC': {'long_name': 'Dissolved inorganic carbon',\n",
    "            'standard_name': 'DIC',\n",
    "            'units': 'micro mol/kg',\n",
    "            'unit_long': 'micro mol/kg'},\n",
    "    'TA': {'long_name': 'Total alkalinity',\n",
    "            'standard_name': 'TA',\n",
    "            'units': 'micro mol/kg',\n",
    "            'unit_long': 'micro mol/kg'},    \n",
    "    'pCO2': {'long_name': 'Seawater partial pressure of CO2',\n",
    "            'standard_name': 'pCO2',\n",
    "            'units': 'micro atm',\n",
    "            'unit_long': 'micro atm'},\n",
    "    'fCO2': {'long_name': 'Seawater fugacity of CO2',\n",
    "            'standard_name': 'fCO2',\n",
    "            'units': 'micro atm',\n",
    "            'unit_long': 'micro atm'},\n",
    "}\n",
    "\n",
    "cols = ['time', 'longitude', 'latitude', 'depth', 'bottom_depth', 'Temperature', 'Salinity', 'pCO2_monthave']\n",
    "outdir = '/bali/data/ilima/GLORYS12V1/daily/BGC/surface'\n",
    "\n",
    "for year in notebook.tqdm(range(1993,2019)):\n",
    "    with xr.open_dataset('/bali/data/ilima/GLORYS12V1/daily/GLORYS12V1_NW_Atlantic_{}_daily.nc'.format(year),\n",
    "                            drop_variables = ['mlotst','zos','bottomT']) as ds_in:\n",
    "        ds = ds_in.isel(depth=0).squeeze(drop=True) # surface fields\n",
    "        # add monthly atmospheric pCO2\n",
    "        for i in df_pco2_monthly.loc[year].index:\n",
    "            if i==1:\n",
    "                fill = np.nan\n",
    "            else:\n",
    "                fill = ds.pCO2_monthave\n",
    "\n",
    "            ds['pCO2_monthave'] = xr.where(ds.time.dt.month==i, df_pco2_monthly.loc[(2016,i),'average'], fill)\n",
    "\n",
    "        # merge bottom depth with GLORYS12V1 data\n",
    "        df_glorys = ds[['pCO2_monthave','thetao','so']].to_dataframe()\n",
    "        df_glorys = df_glorys.reset_index().rename(columns={'thetao':'Temperature', 'so':'Salinity'})\n",
    "        df_glorys = pd.merge(df_positions, df_glorys, on=['longitude', 'latitude'])\n",
    "        df_data = df_glorys[cols].dropna()\n",
    "        # print('{:,d} data points\\n'.format(df_data.shape[0]))\n",
    "\n",
    "        X_numpy = df_data[features].values                      # select features\n",
    "        X_numpy_scaled = scaler.transform(X_numpy)              # rescale features\n",
    "        X = torch.from_numpy(X_numpy_scaled.astype(np.float32)) # convert array to tensor\n",
    "\n",
    "        # apply model to rescaled features\n",
    "        with torch.no_grad():\n",
    "            Y_pred = nn_reg(X)\n",
    "\n",
    "        # add estimated DIC & TA to features dataframe\n",
    "        df_data['DIC'] = Y_pred[:,0]\n",
    "        df_data['TA'] = Y_pred[:,1]\n",
    "\n",
    "        # compute additional carbonate chemistry variables\n",
    "        pressure =  gsw.p_from_z(-df_data.depth.values, df_data.latitude.values)\n",
    "        kwargs = dict(\n",
    "            par1 = df_data.TA.values,   # TA\n",
    "            par2 = df_data.DIC.values,  # DIC\n",
    "            par1_type = 1,              # type 1 = alkalinity\n",
    "            par2_type = 2,              # type 2 = DIC\n",
    "            salinity = df_data.Salinity.values,\n",
    "            temperature = df_data.Temperature.values,\n",
    "            pressure = pressure,\n",
    "            opt_k_carbonic = 10,  # LDK00, Lueker et al 2000\n",
    "            opt_k_bisulfate = 1,  # D90a, Dickson 1990\n",
    "            opt_total_borate = 2, # LKB10, Lee et al 2010\n",
    "            opt_k_fluoride = 2    # PF87, Perez & Fraga 1987\n",
    "        )\n",
    "        results = pyco2.sys(**kwargs)\n",
    "        co2sys_vars = ['pCO2', 'fCO2']\n",
    "        for vname in co2sys_vars:\n",
    "            df_data[vname] = results[vname]\n",
    "\n",
    "        # merge estimated carbonate chemistry variables to original dataframe\n",
    "        for vname in ['DIC','TA'] + co2sys_vars:\n",
    "            df_glorys[vname] = df_data[vname]\n",
    "\n",
    "        # convert dataframe to xarray dataset\n",
    "        df = df_glorys.set_index(['time','latitude','longitude'])\n",
    "        df = df.rename(columns={'Temperature':'SST', 'Salinity':'SSS'})\n",
    "        ds_out = df[['SST', 'SSS', 'DIC', 'TA', 'pCO2', 'fCO2']].to_xarray()\n",
    "\n",
    "        # set variable attributes\n",
    "        for vname in var_attrs:\n",
    "            ds_out[vname].attrs.update(var_attrs[vname])\n",
    "\n",
    "        # copy variable attributes\n",
    "        for vname in ['latitude','longitude']:\n",
    "            for attr in ['long_name','standard_name','units','unit_long']:\n",
    "                ds_out[vname].attrs[attr] = ds[vname].attrs[attr]\n",
    "\n",
    "        # save dataset to netCDF file\n",
    "        outfile = os.path.join(outdir, 'GLORYS12V1_NW_Atlantic_{}_surf_pCO2.nc'.format(year))\n",
    "        print('writing {}'.format(outfile))\n",
    "        ds_out.to_netcdf(outfile, mode='w', unlimited_dims=['time'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
